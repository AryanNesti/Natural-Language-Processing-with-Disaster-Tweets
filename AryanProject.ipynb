{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import nltk\n",
    "from sklearn import tree\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "import graphviz\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import svm\n",
    "from gensim.models import Word2Vec\n",
    "import os\n",
    "from nltk.stem.porter import *\n",
    "from nltk.corpus import stopwords\n",
    "from stringprep import in_table_a1\n",
    "import string\n",
    "import csv\n",
    "import math\n",
    "import json\n",
    "\n",
    "plt.rcParams[\"figure.figsize\"] = (10,10)\n",
    "\n",
    "\n",
    "      \n",
    "              \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "import nltk\n",
    "# nltk.download('punkt')\n",
    "# nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import *\n",
    "# stemming tool from nltk\n",
    "stemmer = PorterStemmer()\n",
    "# a mapping dictionary that help remove punctuations\n",
    "remove_punctuation_map = dict((ord(char), None) for char in string.punctuation)\n",
    "def get_tokens(text):\n",
    "  # turn document into lowercase\n",
    "  lowers = text.lower()\n",
    "  # remove punctuations\n",
    "  no_punctuation = lowers.translate(remove_punctuation_map)\n",
    "  # tokenize document\n",
    "  tokens = nltk.word_tokenize(no_punctuation)\n",
    "  # remove stop words\n",
    "  filtered = [w for w in tokens if not w in stopwords.words('english')]\n",
    "  # stemming process\n",
    "  stemmed = []\n",
    "  for item in filtered:\n",
    "      stemmed.append(stemmer.stem(item))\n",
    "  # final unigrams\n",
    "  return stemmed\n",
    "\n",
    "\n",
    "def get_dict(fpath):\n",
    "    dictionary = {}\n",
    "    with open(fpath, \"r\") as f:\n",
    "        for i, word in enumerate(f):\n",
    "            dictionary[word.strip()] = i\n",
    "    return dictionary\n",
    "Dict = get_dict(\"dictionary.txt\")\n",
    "# with open('dictionary.txt', 'r') as dict:\n",
    "#   for line in dict:\n",
    "#     l = line.strip('\\n')\n",
    "#     Dict.append(l)\n",
    "\n",
    "def remove(list):\n",
    "  nlist = []\n",
    "  for text in list:\n",
    "    ntext = \"\"\n",
    "    for token in get_tokens(text):\n",
    "      if token in Dict:\n",
    "        ntext += token + \" \"\n",
    "    nlist.append(ntext)\n",
    "  return nlist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class Word2VecVectorizer:\n",
    "  def __init__(self, model):\n",
    "    print(\"Loading in word vectors...\")\n",
    "    self.word_vectors = model\n",
    "    print(\"Finished loading in word vectors\")\n",
    "\n",
    "  def fit(self, data):\n",
    "    pass\n",
    "\n",
    "  def transform(self, data):\n",
    "    # determine the dimensionality of vectors\n",
    "    v = self.word_vectors.get_vector('king')\n",
    "    self.D = v.shape[0]\n",
    "\n",
    "    X = np.zeros((len(data), self.D))\n",
    "    n = 0\n",
    "    emptycount = 0\n",
    "    for sentence in data:\n",
    "      tokens = sentence.split()\n",
    "      vecs = []\n",
    "      m = 0\n",
    "      for word in tokens:\n",
    "        try:\n",
    "          # throws KeyError if word not found\n",
    "          vec = self.word_vectors.get_vector(word)\n",
    "          vecs.append(vec)\n",
    "          m += 1\n",
    "        except KeyError:\n",
    "          pass\n",
    "      if len(vecs) > 0:\n",
    "        vecs = np.array(vecs)\n",
    "        X[n] = vecs.mean(axis=0)\n",
    "      else:\n",
    "        emptycount += 1\n",
    "      n += 1\n",
    "    print(\"Numer of samples with no words found: %s / %s\" % (emptycount, len(data)))\n",
    "    return X\n",
    "\n",
    "\n",
    "  def fit_transform(self, data):\n",
    "    self.fit(data)\n",
    "    return self.transform(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/tt/y2sg92b14ll03_4m7_0y9zfh0000gn/T/ipykernel_50407/2500312473.py:16: DeprecationWarning: Call to deprecated `glove2word2vec` (KeyedVectors.load_word2vec_format(.., binary=False, no_header=True) loads GLoVE text vectors.).\n",
      "  glove2word2vec(glove_path, word2vec_output_file)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Our Deeds are the Reason of this #earthquake M...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Forest fire near La Ronge Sask. Canada</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>All residents asked to 'shelter in place' are ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13,000 people receive #wildfires evacuation or...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Just got sent this photo from Ruby #Alaska as ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7608</th>\n",
       "      <td>10869</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Two giant cranes holding a bridge collapse int...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7609</th>\n",
       "      <td>10870</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>@aria_ahrary @TheTawniest The out of control w...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7610</th>\n",
       "      <td>10871</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>M1.94 [01:04 UTC]?5km S of Volcano Hawaii. htt...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7611</th>\n",
       "      <td>10872</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Police investigating after an e-bike collided ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7612</th>\n",
       "      <td>10873</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>The Latest: More Homes Razed by Northern Calif...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7613 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         id keyword location  \\\n",
       "0         1     NaN      NaN   \n",
       "1         4     NaN      NaN   \n",
       "2         5     NaN      NaN   \n",
       "3         6     NaN      NaN   \n",
       "4         7     NaN      NaN   \n",
       "...     ...     ...      ...   \n",
       "7608  10869     NaN      NaN   \n",
       "7609  10870     NaN      NaN   \n",
       "7610  10871     NaN      NaN   \n",
       "7611  10872     NaN      NaN   \n",
       "7612  10873     NaN      NaN   \n",
       "\n",
       "                                                   text  target  \n",
       "0     Our Deeds are the Reason of this #earthquake M...       1  \n",
       "1                Forest fire near La Ronge Sask. Canada       1  \n",
       "2     All residents asked to 'shelter in place' are ...       1  \n",
       "3     13,000 people receive #wildfires evacuation or...       1  \n",
       "4     Just got sent this photo from Ruby #Alaska as ...       1  \n",
       "...                                                 ...     ...  \n",
       "7608  Two giant cranes holding a bridge collapse int...       1  \n",
       "7609  @aria_ahrary @TheTawniest The out of control w...       1  \n",
       "7610  M1.94 [01:04 UTC]?5km S of Volcano Hawaii. htt...       1  \n",
       "7611  Police investigating after an e-bike collided ...       1  \n",
       "7612  The Latest: More Homes Razed by Northern Calif...       1  \n",
       "\n",
       "[7613 rows x 5 columns]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load Data\n",
    "\n",
    "root_folder='.'\n",
    "train_data = pd.read_csv(\"train.csv\")\n",
    "test_data = pd.read_csv(\"test.csv\")\n",
    "\n",
    "data_folder_name='data'\n",
    "glove_filename='glove.6B.100d.txt'\n",
    "\n",
    "DATA_PATH = os.path.abspath(os.path.join(root_folder, data_folder_name))\n",
    "glove_path = os.path.abspath(os.path.join(DATA_PATH, glove_filename))\n",
    "\n",
    "from gensim.scripts.glove2word2vec import glove2word2vec\n",
    "\n",
    "word2vec_output_file = glove_filename+'.word2vec'\n",
    "glove2word2vec(glove_path, word2vec_output_file)\n",
    "\n",
    "train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shuffle Data\n",
    "\n",
    "train_data = train_data.sample(frac=1).reset_index(drop=True)\n",
    "test_data = test_data.sample(frac=1).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X/Y Split\n",
    "\n",
    "train_data_x = train_data[\"text\"]\n",
    "trainX = remove(train_data[\"text\"])\n",
    "train_data_y = train_data[\"target\"]\n",
    "\n",
    "test_data_x = test_data[\"text\"]\n",
    "# testX = test_data_x\n",
    "# test_x = test_data_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7613, 21637)\n"
     ]
    }
   ],
   "source": [
    "vectorizer = CountVectorizer()\n",
    "train_data_x = vectorizer.fit_transform(train_data_x)\n",
    "test_data_x = vectorizer.transform(test_data_x)\n",
    "\n",
    "print(train_data_x.shape)\n",
    "# print(test_data_x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading in word vectors...\n",
      "Finished loading in word vectors\n",
      "Numer of samples with no words found: 825 / 7613\n",
      "(7613, 100)\n"
     ]
    }
   ],
   "source": [
    "from gensim.test.utils import common_texts\n",
    "from gensim.models import KeyedVectors\n",
    "\n",
    "# model = Word2Vec(sentences=common_texts, vector_size=100, window=5, min_count=1, workers=4)\n",
    "# word2vec_output_file = glove_filename+'.word2vec'\n",
    "model = KeyedVectors.load_word2vec_format(word2vec_output_file, binary=False)\n",
    "vectorizer = Word2VecVectorizer(model)\n",
    "\n",
    "\n",
    "trainX = vectorizer.fit_transform(trainX)\n",
    "# test_data_x = vectorizer.transform(test_data_x)\n",
    "print(trainX.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Decision Tree\n",
    "# dt = None\n",
    "\n",
    "# def train_dt(depths, train_data_x, train_data_y):\n",
    "#     global dt\n",
    "#     train_acc_all = []\n",
    "#     val_acc_all = []\n",
    "\n",
    "#     kf = KFold(n_splits = 5)\n",
    "\n",
    "#     for d in depths:\n",
    "#         print(f\"Depth: {d}\")\n",
    "#         train_acc = []\n",
    "#         val_acc = []\n",
    "#         for train_index, val_index in kf.split(train_data_x):\n",
    "#             train_x = train_data_x[train_index,:]\n",
    "#             val_x = train_data_x[val_index,:]\n",
    "\n",
    "#             train_y = train_data_y[train_index]\n",
    "#             val_y = train_data_y[val_index]\n",
    "\n",
    "#             dt = tree.DecisionTreeClassifier(max_depth=d, criterion=\"gini\")\n",
    "#             dt.fit(train_x, train_y)\n",
    "\n",
    "#             train_acc.append(dt.score(train_x, train_y))\n",
    "#             val_acc.append(dt.score(val_x, val_y))\n",
    "\n",
    "#         avg_tacc = np.mean(np.array(train_acc))\n",
    "#         avg_vacc = np.mean(np.array(val_acc))\n",
    "\n",
    "#         print(f\"Avg Train Acc: {avg_tacc}\\tAvg Val Acc: {avg_vacc}\")\n",
    "\n",
    "#         train_acc_all.append(avg_tacc)\n",
    "#         val_acc_all.append(avg_vacc)\n",
    "\n",
    "#     return train_acc_all, val_acc_all\n",
    "    \n",
    "\n",
    "# # depths = [10, 25, 50, 75, 100, 125, 150]\n",
    "# dt_depths = [x for x in range(1,152,10)]\n",
    "# dt_tacc, dt_vacc = train_dt(dt_depths, train_data_x, train_data_y)\n",
    "\n",
    "# plt.plot(dt_depths, dt_tacc, marker='.', label=\"Training accuracy\")\n",
    "# plt.plot(dt_depths, dt_vacc, marker='.', label=\"Validation accuracy\")\n",
    "# plt.xlabel('Depth of tree')\n",
    "# plt.ylabel('Accuracy')\n",
    "# plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dot_data = tree.export_graphviz(dt, max_depth=3, filled=True, rounded=True, feature_names=vectorizer.get_feature_names(), class_names=list(set(list(train_data[\"Category\"]))))\n",
    "# graph = graphviz.Source(dot_data)\n",
    "# graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Depth: 10\n",
      "RF Std Train Acc: 0.0\tRF Std Val Acc: 0.0\n",
      "RF Avg Train Acc: 0.996551724137931\tRF Avg Val Acc: 0.804333552199606\n",
      "SVM Std Train Acc: 0.026600985221674867\tSVM Std Val Acc: 0.006237688772160244\n",
      "SVM Avg Train Acc: 0.9699507389162562\tSVM Avg Val Acc: 0.9699507389162562\n",
      "RF Std Train Acc: 0.02519656145722928\tRF Std Val Acc: 0.010186103110720164\n",
      "RF Avg Train Acc: 0.9789819376026273\tRF Avg Val Acc: 0.8043335521996061\n",
      "SVM Std Train Acc: 0.025664268211199312\tSVM Std Val Acc: 0.008862564009647472\n",
      "SVM Avg Train Acc: 0.9711822660098522\tSVM Avg Val Acc: 0.9711822660098522\n",
      "RF Std Train Acc: 0.02512446201123718\tRF Std Val Acc: 0.013851589743500203\n",
      "RF Avg Train Acc: 0.9762889983579639\tRF Avg Val Acc: 0.798161523309258\n",
      "SVM Std Train Acc: 0.02595642252614063\tSVM Std Val Acc: 0.012645092427799771\n",
      "SVM Avg Train Acc: 0.9708538587848933\tSVM Avg Val Acc: 0.9708538587848933\n",
      "RF Std Train Acc: 0.025698911162994262\tRF Std Val Acc: 0.016226266700577318\n",
      "RF Avg Train Acc: 0.9745719708974062\tRF Avg Val Acc: 0.7936184314079799\n",
      "SVM Std Train Acc: 0.0259115967257736\tSVM Std Val Acc: 0.01584233297902187\n",
      "SVM Avg Train Acc: 0.9709166213091591\tSVM Avg Val Acc: 0.9709166213091591\n",
      "RF Std Train Acc: 0.025789243842842462\tRF Std Val Acc: 0.015086265114583015\n",
      "RF Avg Train Acc: 0.9738379963726574\tRF Avg Val Acc: 0.791152779107944\n",
      "SVM Std Train Acc: 0.025966779672120927\tSVM Std Val Acc: 0.014418373277969989\n",
      "SVM Avg Train Acc: 0.9709378611583108\tSVM Avg Val Acc: 0.9709378611583108\n"
     ]
    }
   ],
   "source": [
    "# Random Forest\n",
    "rf = None\n",
    "dt = None\n",
    "def train_rf(depths, train_data_x, train_data_y):\n",
    "    global rf\n",
    "    global dt\n",
    "    rf_train_acc_all = []\n",
    "    rf_val_acc_all = []\n",
    "    svm_train_acc_all = []\n",
    "    svm_val_acc_all = []\n",
    "    rf_train_std_all = []\n",
    "    rf_val_std_all = []\n",
    "    svm_train_std_all = []\n",
    "    svm_val_std_all = []\n",
    "\n",
    "    kf = KFold(n_splits = 5)\n",
    "\n",
    "    for d in depths:\n",
    "        print(f\"Depth: {d}\")\n",
    "        train_acc = []\n",
    "        val_acc = []\n",
    "        for train_index, val_index in kf.split(train_data_x):\n",
    "            train_x = train_data_x[train_index,:]\n",
    "            val_x = train_data_x[val_index,:]\n",
    "\n",
    "            train_y = train_data_y[train_index]\n",
    "            val_y = train_data_y[val_index]\n",
    "\n",
    "            rf = RandomForestClassifier(n_estimators=100, criterion=\"gini\", max_depth=None, min_samples_split=2, min_samples_leaf=1)\n",
    "            rf.fit(train_x, train_y)\n",
    "\n",
    "            train_acc.append(rf.score(train_x, train_y))\n",
    "            val_acc.append(rf.score(val_x, val_y))\n",
    "\n",
    "            avg_tacc = np.mean(np.array(train_acc))\n",
    "            avg_vacc = np.mean(np.array(val_acc))\n",
    "            std_tacc = np.std(np.array(train_acc))\n",
    "            std_vacc = np.std(np.array(val_acc))\n",
    "            print(f\"RF Std Train Acc: {std_tacc}\\tRF Std Val Acc: {std_vacc}\")\n",
    "            print(f\"RF Avg Train Acc: {avg_tacc}\\tRF Avg Val Acc: {avg_vacc}\")\n",
    "\n",
    "            rf_train_acc_all.append(avg_tacc)\n",
    "            rf_val_acc_all.append(avg_vacc)\n",
    "            rf_train_std_all.append(std_tacc)\n",
    "            rf_val_std_all.append(std_vacc)\n",
    "    \n",
    "\n",
    "            dt = svm.SVC()\n",
    "            dt.fit(train_x, train_y)\n",
    "            \n",
    "            train_acc.append(dt.score(train_x, train_y))\n",
    "            val_acc.append(dt.score(val_x, val_y))\n",
    "\n",
    "            avg_tacc = np.mean(np.array(train_acc))\n",
    "            avg_vacc = np.mean(np.array(train_acc))\n",
    "            std_tacc = np.std(np.array(train_acc))\n",
    "            std_vacc = np.std(np.array(val_acc))\n",
    "            print(f\"SVM Std Train Acc: {std_tacc}\\tSVM Std Val Acc: {std_vacc}\")\n",
    "            print(f\"SVM Avg Train Acc: {avg_tacc}\\tSVM Avg Val Acc: {avg_vacc}\")\n",
    "\n",
    "            svm_train_acc_all.append(avg_tacc)\n",
    "            svm_val_acc_all.append(avg_vacc)\n",
    "            svm_train_std_all.append(std_tacc)\n",
    "            svm_val_std_all.append(std_vacc)\n",
    "\n",
    "    return rf_train_acc_all, rf_val_acc_all, svm_train_acc_all, svm_val_acc_all, rf_train_std_all, rf_val_std_all, svm_train_std_all, svm_val_std_all\n",
    "    \n",
    "\n",
    "# depths = [10, 25, 50, 75, 100, 125, 150]\n",
    "rf_depths = [10]\n",
    "rf_tacc, rf_vacc, svm_tacc, svm_vacc, rf_tstd, rf_vstd, svm_tstd, svm_vstd= train_rf(rf_depths, train_data_x, train_data_y)\n",
    "# trainx = [train_data_x, test_data]\n",
    "# fig, axs = plt.subplots(2, 2)\n",
    "# fig, ax = plt.subplots(2, 2)\n",
    "# for i in range(2):\n",
    "#     rf_depths = [10]\n",
    "#     rf_tacc, rf_vacc, svm_tacc, svm_vacc, rf_tstd, rf_vstd, svm_tstd, svm_vstd= train_rf(rf_depths, trainx[i], train_data_y)\n",
    "#     z = \"Word2Vec\"\n",
    "#     if(i == 0):\n",
    "#         z = \"CountVec\"\n",
    "\n",
    "#     x = np.arange(5)\n",
    "#     axs[i, 0].bar(x, rf_tacc, width = 0.4, label=z + \"Training accuracy\")\n",
    "#     axs[i, 0].bar(x + 0.4, rf_vacc, width = 0.4, label=z + \"Validation accuracy\")\n",
    "#     axs[i, 0].set_title('Random Forrest', color='red')\n",
    "#     axs[i, 0].xaxis.label.set_color('red')\n",
    "#     axs[i, 0].yaxis.label.set_color('red')\n",
    "#     axs[i, 0].tick_params(axis='x', colors='red')\n",
    "#     axs[i, 0].tick_params(axis='y', colors='red')\n",
    "#     axs[i, 0].legend()\n",
    "\n",
    "\n",
    "#     axs[i, 1].bar(x, svm_tacc, width = 0.4, label=z + \"Training accuracy\")\n",
    "#     axs[i, 1].bar(x + 0.4, svm_vacc, width = 0.4, label= z + \"Validation accuracy\")\n",
    "#     axs[i, 1].xaxis.label.set_color('red')\n",
    "#     axs[i, 1].yaxis.label.set_color('red')\n",
    "#     axs[i, 1].tick_params(axis='x', colors='red')\n",
    "#     axs[i, 1].tick_params(axis='y', colors='red')\n",
    "#     axs[i, 1].set_title('SVM', color='red')\n",
    "#     axs[i, 1].legend()\n",
    "\n",
    "#     ax[i, 0].bar(x, rf_tstd, width = 0.4, label=z + \"Training std\")\n",
    "#     ax[i, 0].bar(x + 0.4, rf_vstd, width = 0.4, label=z + \"Validation std\")\n",
    "#     ax[i, 0].set_title('Random Forrest', color='red')\n",
    "#     ax[i, 0].xaxis.label.set_color('red')\n",
    "#     ax[i, 0].yaxis.label.set_color('red')\n",
    "#     ax[i, 0].tick_params(axis='x', colors='red')\n",
    "#     ax[i, 0].tick_params(axis='y', colors='red')\n",
    "#     ax[i, 0].legend()\n",
    "\n",
    "\n",
    "#     ax[i, 1].bar(x, svm_tstd, width = 0.4, label=z + \"Training std\")\n",
    "#     ax[i, 1].bar(x + 0.4, svm_vstd, width = 0.4, label= z + \"Validation std\")\n",
    "#     ax[i, 1].xaxis.label.set_color('red')\n",
    "#     ax[i, 1].yaxis.label.set_color('red')\n",
    "#     ax[i, 1].tick_params(axis='x', colors='red')\n",
    "#     ax[i, 1].tick_params(axis='y', colors='red')\n",
    "#     ax[i, 1].set_title('SVM', color='red')\n",
    "#     ax[i, 1].legend()\n",
    "#     i += 1\n",
    "\n",
    "# for ax in axs.flat:\n",
    "#     ax.set(xlabel='Fold', ylabel='Accuracy')\n",
    "# for axs in axs.flat:\n",
    "#     axs.set(xlabel='Fold', ylabel='Std')\n",
    "\n",
    "# plt.savefig('Chart', edgecolor=\"none\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Gradient Boosting Trees\n",
    "\n",
    "# gbt = None\n",
    "\n",
    "# def train_gbt(depths, train_data_x, train_data_y):\n",
    "#     global gbt\n",
    "#     train_acc_all = []\n",
    "#     val_acc_all = []\n",
    "\n",
    "#     kf = KFold(n_splits = 5)\n",
    "\n",
    "#     for d in depths:\n",
    "#         print(f\"Depth: {d}\")\n",
    "#         train_acc = []\n",
    "#         val_acc = []\n",
    "#         for train_index, val_index in kf.split(train_data_x):\n",
    "#             train_x = train_data_x[train_index,:]\n",
    "#             val_x = train_data_x[val_index,:]\n",
    "\n",
    "#             train_y = train_data_y[train_index]\n",
    "#             val_y = train_data_y[val_index]\n",
    "\n",
    "#             gbt = GradientBoostingClassifier(n_estimators=25, criterion=\"squared_error\", max_depth=d)\n",
    "#             gbt.fit(train_x, train_y)\n",
    "\n",
    "#             train_acc.append(gbt.score(train_x, train_y))\n",
    "#             val_acc.append(gbt.score(val_x, val_y))\n",
    "\n",
    "#         avg_tacc = np.mean(np.array(train_acc))\n",
    "#         avg_vacc = np.mean(np.array(val_acc))\n",
    "\n",
    "#         print(f\"Avg Train Acc: {avg_tacc}\\tAvg Val Acc: {avg_vacc}\")\n",
    "\n",
    "#         train_acc_all.append(avg_tacc)\n",
    "#         val_acc_all.append(avg_vacc)\n",
    "\n",
    "#     return train_acc_all, val_acc_all\n",
    "    \n",
    "\n",
    "# gbt_depths = [10, 25, 50, 75, 100, 125, 150]\n",
    "# gbt_tacc, gbt_vacc = train_gbt(gbt_depths, train_data_x, train_data_y)\n",
    "\n",
    "# plt.plot(gbt_depths, gbt_tacc, marker='.', label=\"Training accuracy\")\n",
    "# plt.plot(gbt_depths, gbt_vacc, marker='.', label=\"Validation accuracy\")\n",
    "# plt.xlabel('Depth of tree')\n",
    "# plt.ylabel('Accuracy')\n",
    "# plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "ret = dt.predict(test_data_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Pred</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>8834</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5811</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8845</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9138</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7181</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6957</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8281</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1913</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5250</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9340</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3263 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Pred\n",
       "id        \n",
       "8834     0\n",
       "5811     1\n",
       "8845     0\n",
       "9138     1\n",
       "7181     0\n",
       "...    ...\n",
       "6957     0\n",
       "8281     1\n",
       "1913     0\n",
       "5250     0\n",
       "9340     0\n",
       "\n",
       "[3263 rows x 1 columns]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "id_pred = list(zip(list(test_data[\"id\"]), list(ret)))\n",
    "id_pred = pd.DataFrame()\n",
    "id_pred[\"id\"] = test_data[\"id\"]\n",
    "id_pred[\"Pred\"] = list(ret)\n",
    "id_pred.set_index(\"id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # SVM\n",
    "# dt = None\n",
    "\n",
    "# def train_dt(depths, train_data_x, train_data_y):\n",
    "#     global dt\n",
    "#     train_acc_all = []\n",
    "#     val_acc_all = []\n",
    "\n",
    "#     kf = KFold(n_splits = 5)\n",
    "\n",
    "#     for d in depths:\n",
    "#         print(f\"Fold: {kf}\")\n",
    "#         train_acc = []\n",
    "#         val_acc = []\n",
    "#         for train_index, val_index in kf.split(train_data_x):\n",
    "#             train_x = train_data_x[train_index,:]\n",
    "#             val_x = train_data_x[val_index,:]\n",
    "\n",
    "#             train_y = train_data_y[train_index]\n",
    "#             val_y = train_data_y[val_index]\n",
    "\n",
    "#             dt = svm.SVC()\n",
    "#             dt.fit(train_x, train_y)\n",
    "\n",
    "#             train_acc.append(dt.score(train_x, train_y))\n",
    "#             val_acc.append(dt.score(val_x, val_y))\n",
    "\n",
    "#             avg_tacc = np.mean(np.array(train_acc))\n",
    "#             avg_vacc = np.mean(np.array(val_acc))\n",
    "\n",
    "#             print(f\"Avg Train Acc: {avg_tacc}\\tAvg Val Acc: {avg_vacc}\")\n",
    "\n",
    "#             train_acc_all.append(avg_tacc)\n",
    "#             val_acc_all.append(avg_vacc)\n",
    "\n",
    "#     return train_acc_all, val_acc_all\n",
    "    \n",
    "# kf = [1,2,3,4,5]\n",
    "# # depths = [10, 25, 50, 75, 100, 125, 150]\n",
    "# dt_depths = [x for x in range(1)]\n",
    "# dt_tacc, dt_vacc = train_dt(dt_depths, trainX, train_data_y)\n",
    "\n",
    "\n",
    "\n",
    "# x = np.arange(5)\n",
    "# plt.bar(x, dt_tacc, width = 0.4, label=\"Training accuracy\")\n",
    "# plt.bar(x + 0.4, dt_vacc, width = 0.4, label=\"Validation accuracy\")\n",
    "# plt.xlabel('Depth of tree')\n",
    "# plt.ylabel('Accuracy')\n",
    "# plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "id_pred.to_csv(\"labels.csv\", header=False, index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
